# job settings
job_script="./scripts/distributed/train.sh"

# task settings
model=UnifiedTransformer
task=DialogGeneration

vocab_path="./projects/lic2022/conf/vocab.txt"
spm_model_file="./projects/lic2022/conf/spm.model"
train_file="./projects/lic2022/preprocess_data/train_dial.txt"
valid_file="./projects/lic2022/preprocess_data/dev_dial.txt"
data_format="raw"
file_format="file"
config_path="./projects/lic2022/conf/dial.json"

# training settings
init_params="./projects/lic2022/model_zoo/12L.pretrain"
in_tokens="true"
batch_size=8192
lr=1e-5
warmup_steps=400
weight_decay=0.01

train_args="--max_knowledge_len 256 --max_src_len 128 --max_tgt_len 128 --max_seq_len 512"

num_epochs=10
log_steps=10
validation_steps=200
save_steps=200

log_dir="./projects/lic2022/train_dial/log"
save_path="./projects/lic2022/train_dial/output"
