# job settings
job_script="./scripts/distributed/infer.sh"

# task settings
model=UnifiedTransformer
task=DialogGeneration

vocab_path="./projects/lic2022/conf/vocab.txt"
spm_model_file="./projects/lic2022/conf/spm.model"
infer_file="./projects/lic2022/preprocess_data/test_query_1.txt"
data_format="raw"
file_format="file"
config_path="./projects/lic2022/conf/query.json"

# inference settings
init_params="./projects/lic2022/model_zoo/query_finetune"
in_tokens="false"
batch_size=8

output_name="response"

# top-k sampling(k = 10) and rerank by length-average ppl(20 samples)
# infer_args="--max_src_len 384 --max_tgt_len 128 --max_seq_len 512 --decoding_strategy topk_sampling --num_samples 20 --topk 3 --is_cn true --filter_cross_repetition false"
# top-p sampling(p = 0.9) and rerank by length-average ppl(20 samples)
# infer_args="--decoding_strategy topp_sampling --num_samples 20 --topp 0.9 --is_cn true --filter_cross_repetition false"
# beam_search(beam size = 10)
infer_args="--max_src_len 384 --max_tgt_len 128 --max_seq_len 512 --decoding_strategy beam_search --beam_size 4 --is_cn true --filter_cross_repetition false"

log_steps=10

log_dir="./projects/lic2022/infer_query/log"
save_path="./projects/lic2022/infer_query/output"
